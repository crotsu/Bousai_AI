{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このページは以下のリンクより， google colaboratoryから動作させることができる．\n",
    "- [Open with Colab](https://colab.research.google.com/github/crotsu/Bousai_AI/blob/master/chap3_NeuralNetwork/NeuralNetwork_document.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. はじめに\n",
    "ニューラルネットワークは，脳の神経回路網をモデル化した機械学習の手法である．  \n",
    "近年，深層学習（Deep Learning）の登場により再びニューラルネットワークが注目を集めている．  \n",
    "深層学習は，画像認識，音声認識，自然言語処理など様々な分野で取り入られており，Google社をはじめ， 多くの企業で利用，研究がされている．  \n",
    "本資料では，ニューラルネットワークの基本的な原理， 学習方法について記述する．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 基本構造\n",
    "ニューラルネットワークは，複数のニューロンとニューロン同士を結合するシナプスから構成される.  \n",
    "シナプスは各結合の強さを表す結合重みを持っている.  \n",
    "ニューラルネットワークの図を示す.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/neuralnetwork.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図1.  3層の階層型ニューラルネットワーク\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークは，とても複雑な関数だと思うと理解が早い．  \n",
    "関数なので，入力$x$と，その関数$f(x)$の出力$y$をとる．  \n",
    "さらに，この関数$f(x)$は，合成関数となっており，イメージとしては，$f(x) = u(s(g(x)+h(x)) + t(m(x)+n(x)) )$のように複雑な合成関数である．  \n",
    "各関数にはパラメータがあり，これをうまく調整すると，ある入力$x$に対して，所望の$y$が出力するようになる．  \n",
    "この調整をトレーニングデータから自動的に行うのがニューラルネットワークである．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/system.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図2.  ニューラルネットワークのイメージ\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークは，入力層，中間層，出力層の多層構造になっており，各ニューロンは前の層のニューロンの出力を入力として受け取り，計算を行ったあと，次のニューロンへと出力する．  \n",
    "各ニューロンの出力が次々と伝播していき，出力層のニューロンからネットワークの出力を得る．  \n",
    "単体のニューロンの構造を図に示す．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/neuron.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図3.   単体のニューロン\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューロンは複数の入力を受け取り，1つの値を出力する．  \n",
    "ある層の上位ニューロン$j$は，その前の層の$I$個の下位ニューロン出力$o_i$を入力として受け取る．  \n",
    "このとき，ニューロン間の重み$w_ij$と入力$o_i$の積和計算する．  \n",
    "そして，閾値$\\theta_j$を加えた値を重み付き和$X_j$として算出する．  \n",
    "その重み付き和の値を活性化関数$f(x)$の引数として，活性化関数の値をニューロン$j$の出力とする．  \n",
    "これらを式で表すと以下のようになる．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "X_j =  \\sum_{i=1}^{I} w_{ij} o_i + \\theta_j \\\\\n",
    "o_j =  f(X_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数には，以下のシグモイド関数や双曲線正接関数などが使われる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x) = \\frac{1}{1 + \\exp(-\\epsilon x)} \\\\\n",
    "f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/sigmoid.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図4.   シグモイド関数と双曲線正接関数のグラフ\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで$\\epsilon$はシグモイド関数の$x=0$のときの傾きを表している．  \n",
    "$\\epsilon$が小さいほど傾きが緩やかになり，大きいほど傾きが傾きが大きくなる．  \n",
    "傾きが急なシグモイド関数ほど，少しの変化で0あるいは1の出力となるので，反応が敏感な関数であると言える．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/sigmoid2.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図5.   シグモイド関数と双曲線正接関数のグラフ\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "近年，これらの活性化関数の代わりに正規化線形関数(Rectified linear function)が利用されることが多くなっている．  \n",
    "一般的には，ReLUと呼ばれる．  \n",
    "ReLUは，\n",
    "\n",
    "- $\\max(0,x)$は単純ゆえに早い\n",
    "- 0を作るので，スパース性につながる\n",
    "- $x>0$の部分では微分値が常に1であるため勾配消失の心配はない  \n",
    "\n",
    "という利点を持つ．  \n",
    "ReLUの式とグラフを以下に示す．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x) = \\max(0,x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/relu.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図6.   ReLU関数\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上の計算を入力層側のニューロンから順に行う．  \n",
    "ただし，入力層のニューロンはこれらの計算を行わず，入力をそのまま出力する．  \n",
    "つまり，入力層2個，中間層2個，出力層1のネットワークは，厳密は以下の図のようになる．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/ff1.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図7.  本来，入力層にはニューロンはない\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しかし，イラストのバランスが悪いので，入力ニューロンは描いてネットワークを表現することが多い．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/ff2.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図8.  バランスが悪いので入力ニューロンは描くことが多い\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この前向きの計算により，入力層への入力に対し，中間層を経て，出力層から出力を得ることができる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 学習の目的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークは，理想の出力に近いほど性能が高いといえる．  \n",
    "各出力ニューロン$k$に対応する教師信号$t_k$が与えられたとき，各入力パターンに対する誤差$E_p$とP個の入力パターンの誤差の総和$E_{all}$は以下の式で表すことができる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E_p = \\frac{1}{2}\\sum_{k = 1}^{K}(o_k - t_k)^2 \\\\\n",
    "E_{all}  =  \\sum_{p = 1}^{P} E_p\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークの学習は，この誤差総和$E_{all}$を最小化し，入力に対して適切な出力が得られるように各結合重みと閾値を調整することである．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 仮想ニューロン"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークの出力を最適化するには，各結合重みと閾値を調整すれば良いが，それぞれを修正するのは手間がかかる．  \n",
    "そこで閾値を重みとして扱うために仮想ニューロンを導入する．  \n",
    "入力層，中間層，出力層のニューロン数がそれぞれ$I$個，$J$個，$K$個のニューラルネットワークに仮想ニューロンを導入した形を図に示す．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/virtual.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図9.  仮想ニューロン\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仮想ニューロンは出力層を除く，各層のニューロンの最後に追加する．  \n",
    "図のニューラルネットワークでは，$I + 1$番目，$J + 1$番目に追加している．  \n",
    "各仮想ニューロンは次の層の全ニューロンと結合しており，結合先のニューロンの閾値を結合重みとしている．  \n",
    "また，各仮想ニューロンは常に$1$を出力している．これにより，重み付け和の式は以下のように置き換えられる．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{eqnarray}\n",
    "\tX_j & = & \\sum_{i = 1}^{I} w_{ij} o_i - \\theta_j \\nonumber \\\\\n",
    "\t& = & \\sum_{i = 1}^{I} w_{ij} o_i + w_{(I + 1), j} \\nonumber \\\\\n",
    "\t& = & \\sum_{i = 1}^{I} w_{ij} o_i + w_{(I + 1), j} \\cdot 1 \\nonumber \\\\\n",
    "\t& = & \\sum_{i = 1}^{I} w_{ij} o_i + w_{(I + 1), j} \\cdot o_{I + 1} \\nonumber \\\\\n",
    "\t& = & \\sum_{i = 1}^{I + 1} w_{ij} o_i\n",
    "\t\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この式変形により，閾値も重みとして扱うことができる．  \n",
    "これにより，この後のの重みを修正する式によって，間接的に閾値も修正することができる．  \n",
    "ただし，閾値がなくなったわけではないので，実装のときには注意が必要である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 非線形問題と線形問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 問題の困難度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向き計算の例を考えてみよう．  \n",
    "例えば，魚Aと魚Bを仕分けするシステムを作りたいとする．  \n",
    "ベルトコンベアの乗って流れてきた魚の体長と体重を測って，それらから魚を推定する．  \n",
    "体長をx, 体重をyとすると2次元平面上にマッピングできる．  \n",
    "図の左のようになる．  \n",
    "これを見ると，青の魚Aと赤の魚Bが入り組んでいて，分類が難しそうである．  \n",
    "\n",
    "一方，簡単な例として図の右のようなものを考えてみよう．  \n",
    "この図では，きれいに分けることができそうである．  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/fish1.png\" width=\"800\">\n",
    "<div style=\"text-align: center;\">\n",
    "図10.  魚の分類\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なぜ，左側の図の方が難しいのだろうか．  \n",
    "それは，以下の図のように，右側の問題は直線で分離が可能な線形問題だからである．  \n",
    "線形分離とは，元の次元$n$から1次引いた$n-1$次元の超平面で分離できるかどうかである．  \n",
    "今，2次元（体長と体重）の特徴ベクトルなので2次元平面を分離する超平面は1次元の直線である．  \n",
    "一方，例えば3次元のデータを分離するのは，2次元の平面ということになる．  \n",
    "同様に，$n$次元のデータを分離するのは，$n-1$次元の超平面ということがいえる．  \n",
    "つまり，何次元のデータであっても超平面で分離できれば線形分離可能な問題ということになる．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/fish2.png\" width=\"800\">\n",
    "<div style=\"text-align: center;\">\n",
    "図11.  魚の分類\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "魚の分類（難しい）の問題は非線形問題である．  \n",
    "非線形問題で一番簡単な問題はXOR（排他的論理和）の問題である．  \n",
    "XORは4つのデータしかないが，これを1次元の直線で分離させることはできない．  \n",
    "そこで，ここでは，まず非線形問題で一番簡単なXORを学習させてみよう．  \n",
    "逆に言えば，XORが学習できれば，本質が同じ魚の分類もできる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/fish3.png\" width=\"800\">\n",
    "<div style=\"text-align: center;\">\n",
    "図12.  魚の分類\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 XORを学習したニューラルネットワークの前向き計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 以下の図は，すでにXORを学習済みの重みと閾値のニューラルネットワークである．  \n",
    " 例えば，(0,1)を入力すると，どうなるか計算してみよう．  \n",
    " 正しく，1と出力されるはずである．  \n",
    " では，他の3パターンはどうだろうか？  \n",
    " いちいち手計算するのは大変なので，プログラムで動かしてみよう．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/xor1.png\" width=\"800\">\n",
    "<div style=\"text-align: center;\">\n",
    "図13.  XORの例\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. デモ（前向き計算）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "プログラムで動かすために，設計が必要であるが，ニューラルネットワークを理解をすることを最優先して，  \n",
    "「拡張性ゼロのアホアホプログラム」  \n",
    "で体験することにしよう．  \n",
    "（配列や複雑な関数で構成されたプログラムは，ややこしくて本質が見えなくなります）  \n",
    "\n",
    "では，どうするか？\n",
    "各重みは，全部変数にしてしまおう！！！  \n",
    "（なんと乱暴な！しかし，とりあえず理解するには，これが一番わかりやすい！）  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/xor2.png\" width=\"500\">\n",
    "<div style=\"text-align: center;\">\n",
    "図14.  XORの例\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューロンAとニューロンB間の重みはwab  \n",
    "ニューロンAの閾値はtha  \n",
    "のように変数で表現します．  \n",
    "（wはweight，thはthresholdの頭文字です）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[前向き計算（拡張性ゼロのアホアホプログラム）](./chap3_1_forward.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習\n",
    "このプログラムのコメントアウト部分である，ランダムに重みと閾値を与える部分を実行すると，  \n",
    "未学習なのでXORの出力にはならないことを確認してみよう．  \n",
    "ニューラルネットワークは，初期重みと閾値をランダムに与えて，学習によって最適化する．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラルネットワークは，でっかい複雑な「ただの関数」\n",
    "\n",
    "上記のパラメータを展開すると，2入力1出力の関数になる．  \n",
    "数学の関数$y=f(x_1, x_2)$と同じである．  \n",
    "したがって，何か2入力を与えると，何か出力される．  \n",
    "<img src=\"img/expand1.png\" width=\"500\">\n",
    "<div style=\"text-align: center;\">\n",
    "図15.  2入力1出力のニューラルネットワーク\n",
    "</div>\n",
    "\n",
    "これを入力に対して，適切な出力がなされるように，パラメータ（重みと閾値）を求める．  \n",
    "ニューラルネットワークは，ただの関数だということを理解することが大事である．\n",
    "\n",
    "<img src=\"img/expand2.png\" width=\"500\">\n",
    "<div style=\"text-align: center;\">\n",
    "図16.  2入力1出力の「ただの関数」\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちなみに，愚直にプログラムを数式にすると以下のようになる．  \n",
    "\n",
    "$$\n",
    "g(x) = \\frac{1}{1+\\exp(-4.0x))}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = g(1.94 \\times g(-1.54 x_1 + 1.60 x_2 - 0.92) -1.88 \\times g(-1.21 x_1 + 1.29 x_2 + 0.58) + 0.88)\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[式を展開したプログラム](./chap3_2_expand.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. バックプロパゲーション法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最急降下法を用いて図のようにニューラルネットワークの出力層から入力層に向かって重みの調整を行う手法を誤差逆伝播法（Back Propagation）という．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/bp1.png\" width=\"500\">\n",
    "<div style=\"text-align: center;\">\n",
    "図17.  BP法の修正の方向\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習の目的は誤差総和$E_{all}$を最小化することだが$E_{all}$を最小化することは困難であるため，  \n",
    "各パターンの誤差$E_p$が最小となるように重みを更新する．  \n",
    "重みが変化すると出力と誤差も変化するため，  \n",
    "$E_p$は$w$を引数とした関数$E_p(w)$と考えることができる．  \n",
    "したがって，  $\\frac{\\partial E_p(w)}{\\partial w}$を求めて，$w$を更新することで$E_p$を小さくすることができる．  \n",
    "\n",
    "最急降下法を用いた重み$w$の更新式は以下のようになる．  \t\n",
    "$$\n",
    "w_{t+1} = w_t - \\eta \\frac{\\partial E_p(w_t)}{\\partial w_t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この最急降下法を用いて各重みを出力層から入力層に向かって順に更新していく．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/bp2.png\" width=\"500\">\n",
    "<div style=\"text-align: center;\">\n",
    "図18.  BP法導出するための記号\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初に中間層-出力層の任意の重み$w_{jk}$について考える．  \n",
    "$E_p$の式には直接$w_{jk}$は存在しないので合成関数の微分を考える．  \n",
    "$\\frac{\\partial E_p}{\\partial w_{jk}}$の合成関数の微分は以下のようになる．\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_p}{\\partial w_{jk}} = \\frac{\\partial E_p}{\\partial o_k}\n",
    "\\frac{\\partial o_k}{\\partial X_k} \\frac{\\partial X_k}{\\partial w_{jk}}\n",
    "$$\n",
    "\n",
    "これらの偏微分をそれぞれ計算すると以下のようになる．\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_p}{\\partial o_k}  =  \\frac{1}{2}\\{2(o_k - t_k)\\}\n",
    "\t\t= o_k - t_k \\\\\n",
    "\\frac{\\partial o_k}{\\partial X_k}  =  f'(X_k) \\\\\n",
    "\\frac{\\partial X_k}{\\partial w_{jk}}  =  o_j \n",
    "$$\n",
    "\t\n",
    "以上より，中間層-出力層の重みの更新式は以下のようになる．\n",
    "\t\n",
    "$$\n",
    "w_{jk} = w_{jk} - \\eta (o_k - t_k) f'(X_k) o_j\n",
    "$$\n",
    "\t\n",
    "さらに出力ニューロンに関係する式を$\\delta_k$としてまとめると，この式は次のように表せる．\n",
    "\n",
    "$$\n",
    "\\delta_k = \\frac{\\partial E_p}{\\partial o_k} \\frac{\\partial o_k}{\\partial X_k} = (o_k - t_k)f'(X_k) \\\\\n",
    "w_{jk} = w_{jk} - \\eta \\delta_k o_j\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に中間層以下の重み$w_{ij}$について考える．  \n",
    "中間層以下の重みの合成関数の微分は以下のようになる．\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_p}{\\partial w_{ij}}  = \\sum_{k = 1}^{K} \\frac{\\partial E_p}{\\partial o_k} \\frac{\\partial o_k}{\\partial X_k} \\frac{\\partial X_k}{\\partial o_j} \\frac{\\partial o_j}{\\partial X_j} \\frac{\\partial X_j}{\\partial w_{ij}}\n",
    "$$\n",
    "\n",
    "この式では，注目ニューロンより上位の層で結合している全てのニューロンの誤差を考慮するために$\\sum$を取っている．  \n",
    "この式の$\\frac{\\partial E_p}{\\partial o_k} \\frac{\\partial o_k}{\\partial X_k}$は上で求めた$\\delta_k$が代入できるので，  \n",
    "それ以外の微分について考える．  \n",
    "各偏微分は以下のように計算できる．\n",
    "\n",
    "$$\n",
    "\\frac{\\partial X_k}{\\partial o_j} = w_{jk} \\\\\n",
    "\\frac{\\partial o_j}{\\partial X_j} = f'(X_j) \\\\\n",
    "\\frac{\\partial X_j}{\\partial w_{ij}} = o_i \n",
    "$$\n",
    "\n",
    "以上より，中間層以下の重みの更新式は以下のようになる．\n",
    "\n",
    "$$\n",
    "\\delta_j  =  \\sum_{k = 1}^{K} \\frac{\\partial E_p}{\\partial o_k}\n",
    "\\frac{\\partial o_k}{\\partial X_k} \\frac{\\partial X_k}{\\partial o_j}\n",
    "\\frac{\\partial o_j}{\\partial X_j} = \\sum_{k = 1}^{K} \\delta_k w_{jk} f'(X_j) \\\\\n",
    "w_{ij}  =  w_{ij} - \\eta \\delta_j o_i\n",
    "$$\n",
    "\n",
    "以後，同じように繰り返し$\\delta$を求めていけば，全ての重みを修正することができる．  \n",
    "活性化関数がシグモイド関数の場合，$f'(x)$は以下のようになる．\n",
    "\n",
    "$$\n",
    "f'(x) = \\frac{\\epsilon \\exp(-\\epsilon x)}{(1 + \\exp(-\\epsilon x))^2} \\\\\n",
    "= \\epsilon \\cdot \\frac{\\exp(-\\epsilon x)}{1 + \\exp(-\\epsilon x)} \\cdot \\frac{1}{1 + \\exp(-\\epsilon x)} \\nonumber \\\\\n",
    "= \\epsilon \\cdot \\frac{1 + \\exp(-\\epsilon x) - 1}{1 + \\exp(-\\epsilon x)} \\cdot \\frac{1}{1 + \\exp(-\\epsilon x)} \\nonumber \\\\\n",
    "= \\epsilon \\cdot \\Biggl(1 - \\frac{1}{1 + \\exp(-\\epsilon x)}\\Biggr) \\cdot \\frac{1}{1 + \\exp(-\\epsilon x)} \\nonumber \\\\\n",
    "= \\epsilon (1 - f(x))f(x) \\nonumber \\\\\n",
    "$$\n",
    "\n",
    "またf(X_k) = o_kより  \n",
    "\n",
    "$$\n",
    "f'(X_k) = \\epsilon (1 - o_k)o_k\n",
    "$$\n",
    "\n",
    "と簡単化できる．  \n",
    "\n",
    "このようにシグモイド関数の場合は，各ニューロンの出力から$f'(X_k)$を求めることができる．  \n",
    "以上をまとめると，各重みと閾値の更新式は以下のようになる．\n",
    "\n",
    "- 中間層-出力層\n",
    "$$\n",
    "\\delta_k = (o_k - t_k) \\epsilon (1 - o_k)o_k \\\\\n",
    "w_{jk} = w_{jk} - \\eta \\delta_k o_j \\\\\n",
    "\\theta_{k}  =  \\theta_{k}-\\eta \\delta_k\n",
    "$$\n",
    "\n",
    "- 中間層以下\n",
    "$$\n",
    "\\delta_j = \\sum_{k = 1}^{K} \\delta_k w_{jk} \\epsilon (1 - o_j) o_j \\\\\n",
    "w_{ij} = w_{ij} - \\eta \\delta_j o_i \\\\\n",
    "\\theta_{j} = \\theta_{j}-\\eta \\delta_j\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 補足 （合成関数の偏微分）\n",
    "\n",
    "$z=f(x,y)$が$x,y$で全微分可能であり，$x=x(t), y=y(t)$が$t$で微分可能ならば，  \n",
    "合成関数$z=f(x(t), y(t))$は$t$微分可能であり，次の式が成り立つ．  \n",
    "\n",
    "$$\n",
    "\\frac{dz}{dt}= \\frac{\\partial z}{\\partial x}\\frac{\\partial x}{\\partial t} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial t}\n",
    "$$\n",
    "\n",
    "図でイメージすると，$z$は$x$と$y$の関数であり，$x$と$y$は$t$の関数である．  \n",
    "したがって，$t$から$x,y$への線分と，$x$から$z$，$y$から$z$への線分が必要となる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/bp3.png\" width=\"250\">\n",
    "<div style=\"text-align: center;\">\n",
    "図19.  合成関数の関係\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. デモ（バックプロパゲーション法）\n",
    "\n",
    "先程と同じように拡張性がないプログラムでBP法で重みを学習してみよう．  \n",
    "\n",
    "[バックプロパゲーション法（拡張性ゼロのアホアホプログラム）](./chap3_3_backpropagation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 行列・ベクトル表現でニューラルネットワークを考える\n",
    "\n",
    "ニューロンの説明では以下のようなスカラーを用いた．  \n",
    "$$\n",
    "X_j =  \\sum_{i=1}^{I} w_{ij} o_i + \\theta_j \\\\\n",
    "o_j =  f(X_j)\n",
    "$$\n",
    "\n",
    "これは，行列とベクトルを使うともっと簡単に表現することができる．  \n",
    "\n",
    "まず，下の図でスカラー計算を行ってみよう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/vector1.png\" width=\"500\">\n",
    "<div style=\"text-align: center;\">\n",
    "図20.  行列・ベクトル表現のための変数名\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "X_b = (wbd\\times od + wbe\\times oe) + thb\\\\\n",
    "ob = f(X_b)\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "X_c = (wcd\\times od + wce\\times oe) + thc\\\\\n",
    "ob = f(X_b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では，重みを行列，出力，閾値をベクトルで表現してみよう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "W^{(1)} = \n",
    "\\begin{pmatrix}\n",
    "    wbd & wbe\\\\\n",
    "    wcd & wce\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "o^{(0)} =\n",
    "\\begin{pmatrix}\n",
    "    od\\\\\n",
    "    oe\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "th^{(1)} =\n",
    "\\begin{pmatrix}\n",
    "    thb\\\\\n",
    "    thc\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "すると，行列とベクトルの積で簡単に表現することができる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "X^{(1)}=\n",
    "\\begin{pmatrix}\n",
    "    X_b\\\\\n",
    "    X_c\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "o^{(0)} =\n",
    "\\begin{pmatrix}\n",
    "    od\\\\\n",
    "    oe\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "とおけば，以下のようになる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "X^{(1)} = W^{(1)}o^{(0)}+th^{(1)}\\\\\n",
    "o^{(1)} = f(X^{(1)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpyは行列・ベクトル演算が簡単に行える．  \n",
    "したがって，スカラー表現のプログラムを行列・ベクトル表現のプログラムに変更すれば拡張性が高いプログラムを作成できる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. デモ（行列・ベクトル表現）\n",
    "\n",
    "Numpyを使って行列・ベクトル表現の拡張性の高いプログラムで試してみよう．\n",
    "\n",
    "[行列・ベクトル表現のプログラム](./chap3_4_vector.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. フレームワークを使う(TensorFlow + Keras)\n",
    "\n",
    "ここまで見てきたように，頑張れば自力でニューラルネットワークを実装することができる．  \n",
    "しかし，とても大変だったことがわかると思う．  \n",
    "さらに，今，Deep Learningの研究を行っている人が多数いる．  \n",
    "その人達と成果物を共有しようとするとなると，各自が自作したプログラムでは議論が難しい．  \n",
    "\n",
    "そこで，Deep Learningを動かす枠組み（フレームワーク）が多く開発されている．  \n",
    "ここでは，それらの一つであるGoogleが開発したTensorFlowを紹介する．  \n",
    "TensorFlowはニューラルネットワークを動かすというよりは，テンソル（3次元以上の多次元行列）の演算を行うために開発されたものである．  \n",
    "もちろん，Deep Learningのために開発されたのであるが，ニューラルネットワークは行列の演算を多く行う．  \n",
    "そのため，行列演算のために，しかも並列計算が可能になるように開発されたものがTensorFlowである．  \n",
    "\n",
    "さらに，　KerasはTensorFlowのフロントエンドあり，Deep Learningのモデルを短い記述で実装できる．  \n",
    "一般的に，TensorFlowでDeep Learningを実装しようとすると，Kerasを用いる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. デモ（TensorFlow + Keras）\n",
    "\n",
    "TensorFlow + Kerasを使えば簡単に実装できる．  \n",
    "ニューラルネットワークの構造を変えたり，活性化関数を変更したり，最適化のアルゴリズムを変更したり，とにかくいろいろできる．  \n",
    "さらに，グラフを表示したり，ネットワークの出力を可視化したり，とにかく機能が豊富である．  \n",
    "\n",
    "そのため，せっかく簡単に記述できるにも関わらず，とても複雑になり，初心者にはわかりにくいサンプルプログラムをが多い．  \n",
    "そこで，下記のプログラムはこれまで説明してきたXORを，TensorFlow + Kerasで短く，わかりやすく記述することを目的とする．  \n",
    "\n",
    "[TensorFlow+Keras](./chap3_5_keras.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

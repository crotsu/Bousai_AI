{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. はじめに\n",
    "ニューラルネットワークは，脳の神経回路網をモデル化した機械学習の手法である．近年，深層学習（Deep Learning）の登場により再びニューラルネットワークが注目を集めている．深層学習は，画像認識，音声認識，自然言語処理など様々な分野で取り入られており，Google社をはじめ， 多くの企業で利用，研究がされている．本資料では，ニューラルネットワークの基本的な原理， 学習方法について記述する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 基本構造\n",
    "ニューラルネットワークは，複数のニューロンとニューロン同士を結合するシナプスから構成される.  \n",
    "シナプスは各結合の強さを表す結合重みを持っている.  \n",
    "ニューラルネットワークの図を示す.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/neuralnetwork.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図1.  3層の階層型ニューラルネットワーク\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークは，とても複雑な関数だと思うと理解が早い．  \n",
    "関数なので，入力$x$と，その関数$f(x)$の出力$y$をとる．  \n",
    "さらに，この関数$f(x)$は，合成関数となっており，イメージとしては，$f(x) = u(s(g(x)+h(x)) + t(m(x)+n(x)) )$のように複雑な合成関数である．  \n",
    "各関数にはパラメータがあり，これをうまく調整すると，ある入力$x$に対して，所望の$y$が出力するようになる．  \n",
    "この調整をトレーニングデータから自動的に行うのがニューラルネットワークである．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/system.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図2.  ニューラルネットワークのイメージ\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークは，入力層，中間層，出力層の多層構造になっており，各ニューロンは前の層のニューロンの出力を入力として受け取り，計算を行ったあと，次のニューロンへと出力する．  \n",
    "各ニューロンの出力が次々と伝播していき，出力層のニューロンからネットワークの出力を得る．  \n",
    "単体のニューロンの構造を図に示す．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/neuron.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図3.   単体のニューロン\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューロンは複数の入力を受け取り，1つの値を出力する．  \n",
    "ある層の上位ニューロン$j$は，その前の層の$I$個の下位ニューロン出力$o_i$を入力として受け取る．  \n",
    "このとき，ニューロン間の重み$w_ij$と入力$o_i$の積和計算する．  \n",
    "そして，閾値$\\theta_j$を加えた値を重み付き和$X_j$として算出する．  \n",
    "その重み付き和の値を活性化関数$f(x)$の引数として，活性化関数の値をニューロン$j$の出力とする．  \n",
    "これらを式で表すと以下のようになる．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "X_j =  \\sum_{i=1}^{I} w_{ij} o_i + \\theta_j \\\\\n",
    "o_j =  f(X_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数には，以下のシグモイド関数や双曲線正接関数などが使われる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x) = \\frac{1}{1 + \\exp(-\\epsilon x)} \\\\\n",
    "f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/sigmoid.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図4.   シグモイド関数と双曲線正接関数のグラフ\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで$\\epsilon$はシグモイド関数の$x=0$のときの傾きを表している．  \n",
    "$\\epsilon$が小さいほど傾きが緩やかになり，大きいほど傾きが傾きが大きくなる．  \n",
    "傾きが急なシグモイド関数ほど，少しの変化で0あるいは1の出力となるので，反応が敏感な関数であると言える．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/sigmoid2.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図5.   シグモイド関数と双曲線正接関数のグラフ\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "近年，これらの活性化関数の代わりに正規化線形関数(Rectified linear function)が利用されることが多くなっている．  \n",
    "一般的には，ReLUと呼ばれる．  \n",
    "ReLUは，\n",
    "\n",
    "- $\\max(0,x)$は単純ゆえに早い\n",
    "- 0を作るので，スパース性につながる\n",
    "- $x>0$の部分では微分値が常に1であるため勾配消失の心配はない  \n",
    "\n",
    "という利点を持つ．  \n",
    "ReLUの式とグラフを以下に示す．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x) = \\max(0,x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/relu.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図6.   ReLU関数\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上の計算を入力層側のニューロンから順に行う．  \n",
    "ただし，入力層のニューロンはこれらの計算を行わず，入力をそのまま出力する．  \n",
    "つまり，入力層2個，中間層2個，出力層1のネットワークは，厳密は以下の図のようになる．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/ff1.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図7.  本来，入力層にはニューロンはない\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しかし，イラストのバランスが悪いので，入力ニューロンは描いてネットワークを表現することが多い．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/ff2.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図8.  バランスが悪いので入力ニューロンは描くことが多い\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この前向きの計算により，入力層への入力に対し，中間層を経て，出力層から出力を得ることができる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 学習の目的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークは，理想の出力に近いほど性能が高いといえる．  \n",
    "各出力ニューロン$k$に対応する教師信号$t_k$が与えられたとき，各入力パターンに対する誤差$E_p$とP個の入力パターンの誤差の総和$E_{all}$は以下の式で表すことができる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E_p = \\frac{1}{2}\\sum_{k = 1}^{K}(o_k - t_k)^2 \\\\\n",
    "E_{all}  =  \\sum_{p = 1}^{P} E_p\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークの学習は，この誤差総和$E_{all}$を最小化し，入力に対して適切な出力が得られるように各結合重みと閾値を調整することである．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 仮想ニューロン"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークの出力を最適化するには，各結合重みと閾値を調整すれば良いが，それぞれを修正するのは手間がかかる．  \n",
    "そこで閾値を重みとして扱うために仮想ニューロンを導入する．  \n",
    "入力層，中間層，出力層のニューロン数がそれぞれ$I$個，$J$個，$K$個のニューラルネットワークに仮想ニューロンを導入した形を図に示す．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/virtual.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図9.  仮想ニューロン\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仮想ニューロンは出力層を除く，各層のニューロンの最後に追加する．  \n",
    "図のニューラルネットワークでは，$I + 1$番目，$J + 1$番目に追加している．  \n",
    "各仮想ニューロンは次の層の全ニューロンと結合しており，結合先のニューロンの閾値を結合重みとしている．  \n",
    "また，各仮想ニューロンは常に$1$を出力している．これにより，重み付け和の式は以下のように置き換えられる．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{eqnarray}\n",
    "\tX_j & = & \\sum_{i = 1}^{I} w_{ij} o_i - \\theta_j \\nonumber \\\\\n",
    "\t& = & \\sum_{i = 1}^{I} w_{ij} o_i + w_{(I + 1), j} \\nonumber \\\\\n",
    "\t& = & \\sum_{i = 1}^{I} w_{ij} o_i + w_{(I + 1), j} \\cdot 1 \\nonumber \\\\\n",
    "\t& = & \\sum_{i = 1}^{I} w_{ij} o_i + w_{(I + 1), j} \\cdot o_{I + 1} \\nonumber \\\\\n",
    "\t& = & \\sum_{i = 1}^{I + 1} w_{ij} o_i\n",
    "\t\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この式変形により，閾値も重みとして扱うことができる．  \n",
    "これにより，この後のの重みを修正する式によって，間接的に閾値も修正することができる．  \n",
    "ただし，閾値がなくなったわけではないので，実装のときには注意が必要である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5． 前向き計算のデモ\n",
    "ここまでのところで，ニューラルネットワークの前向き計算ができるようになった．  \n",
    "デモを実行して，確認してみよう．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向き計算の例を考えてみよう．  \n",
    "例えば，魚Aと魚Bを仕分けするシステムを作りたいとする．  \n",
    "ベルトコンベアの乗って流れてきた魚の体長と体重を測って，それらから魚を推定する．  \n",
    "体長をx, 体重をyとすると2次元平面上にマッピングできる．  \n",
    "図の左のようになる．  \n",
    "これを見ると，青の魚Aと赤の魚Bが入り組んでいて，分類が難しそうである．  \n",
    "\n",
    "一方，簡単な例として図の右のようなものを考えてみよう．  \n",
    "この図では，きれいに分けることができそうである．  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/fish1.png\" width=\"800\">\n",
    "<div style=\"text-align: center;\">\n",
    "図10.  魚の分類\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/fish2.png\" width=\"800\">\n",
    "<div style=\"text-align: center;\">\n",
    "図11.  魚の分類\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/fish3.png\" width=\"800\">\n",
    "<div style=\"text-align: center;\">\n",
    "図12.  魚の分類\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/xor1.png\" width=\"800\">\n",
    "<div style=\"text-align: center;\">\n",
    "図13.  XORの例\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/xor2.png\" width=\"500\">\n",
    "<div style=\"text-align: center;\">\n",
    "図14.  XORの例\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のリンクより， 前向き計算を試してみよう．\n",
    "- [chap3_1_forward](https://colab.research.google.com/github/crotsu/Bousai_AI/blob/master/chap3_Python/chap3_1_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. バックプロパゲーション法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最急降下法ってなーに？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このページは以下のリンクより， google colaboratoryから動作させることができる．\n",
    "- [Open with Colab](https://colab.research.google.com/github/crotsu/Bousai_AI/blob/master/chap2_Python/chap2_4_GDM.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この章の最後に最急降下法を行う．\n",
    "\n",
    "最急降下法は，次章のニューラルネットワークの学習でも用いる手法となるので，ここで最急降下法の原理を学ぶ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "1. 最急降下法とは\n",
    "1. 最急降下法を1変数関数に適用する\n",
    "1. 最急降下法を多変数関数に適用する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 最急降下法とは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機械学習では，データが発生するメカニズムをなんらかの数理モデルに当てはめる．  \n",
    "このとき，目的関数を設定し，この目的関数を最小（あるいは最大）にするパラメータを求めることがよくある．  \n",
    "目的関数が解析に解けるときは良いが，陽に解けない場合，その近似値をなんらかの方法で求める必要がある．  \n",
    "このときに用いる手法が最急降下法である．  \n",
    "最急降下法は，Deep Learningの学習にも使われており，機械学習では基本の技術となっている．  \n",
    "最急降下法の式を以下に示す．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_t = x_{t-1} - \\eta \\frac{\\partial f(x_{t-1})}{\\partial x_{t-1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この式の意味は，現在の値$x_{t-1}$に，そのときの関数の傾き$\\frac{\\partial f(x_{t-1})}{\\partial x_{t-1}}$を$\\eta$倍して，引くというものである．  \n",
    "$\\eta$は学習係数と呼ばれ，修正量を調節するパラメータである．  \n",
    "これを繰り返して，極小値を求める手法である．  \n",
    "最急降下法のイメージを以下の図に示す．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gdm1.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図1.  最急降下法のイメージ\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gdm2.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図2.  修正を繰り返して極小値を求めていく様子\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最急降下法の特徴として，傾きが大きいときは，修正量も大きくなるため極小値に一気に近づく．  \n",
    "一方，関数の傾きが小さいと，修正量が小さくなる．  \n",
    "つまり，最急降下法は極小値に近づけば近づくほど，修正量が小さくなり，なかなか近づかなくなる．  \n",
    "理論的には，無限回繰り返して，ようやく極小値に近づくということになる．  \n",
    "しかし，無限回繰り返すことはできないため，近似値を求めて終了する．  \n",
    "近似値の精度の決め方は，一般的には前回と今回の差がある値より小さくなったら，もう修正されなくなったと判断して，繰り返しを打ち切る．  \n",
    "具体的には，\n",
    "$$\n",
    "|x_{t-1}-x_t|<EPS\n",
    "$$\n",
    "とする．  \n",
    "EPSは事前に決めておく精度パラメータである．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gdm3.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図3.  傾きによる修正量の違い\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修正する方向は関数の傾きから決定される．  \n",
    "傾きが正のときは引き算されるので，現在値から見て左側に修正される．  \n",
    "一方，傾きが負のときは足し算になるので，現在値から見て右側に修正されることになる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gdm4.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図4.  修正される方向\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修正量が大きすぎると発散する．  \n",
    "以下の関数$f(x)=x^2$の場合，現在の値が極小値をとる$x=0$の対象点よりも大きく修正されると，次の修正ではさらに修正量が大きくなるため発散する．  \n",
    "このような問題を解決するために，学習係数$\\eta$を小さい値（例えば，0.001など）に設定して回避する．  \n",
    "当然，学習係数$\\eta$を小さくすれば，一回の修正量が小さくなるため，収束するまでに時間がかかる．  \n",
    "しかし，発散するよりは，計算機のパワーで何回も小さく修正して収束するほうが良い結果となることが多い．  \n",
    "\n",
    "近年，この学習係数$\\eta$を適切に決定する手法が多く提案されている．  \n",
    "深層学習ではAdam(Adaptive moment estimation)と呼ばれる手法が用いられることがある．  \n",
    "\n",
    "参考文献:[An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gdm5.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図5.  最急降下法で発散する様子\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 弱点\n",
    "最急降下法は傾きを使って修正する方法であるため極小値しか求めることができない．  \n",
    "初期値によって求まる極小値は変わる．  \n",
    "最小値を求めたい場合，関数の形状を把握した上で初期値を決定しないといけない（基本的には無理．多変数関数の形状はわからないから．）\n",
    "\n",
    "その対策としてディープラーニングでは確率的勾配降下法と呼ばれる手法を用いている．  \n",
    "ディープラーニングでは，トレーニングデータから修正量を求めて学習を行う．  \n",
    "ここでは簡単に説明するが， 一度にすべてのデータによって修正量を求めるのではなく，いくつかのデータで修正量を求めて少しずつ修正する方法である．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メリット\n",
    "微分できれば，多変数関数でも使える．  \n",
    "つまり，現在点よりも必ず良いパラメータが求まる．  \n",
    "\n",
    "例えば，AI将棋の先駆けとなったBonanzaでは評価関数の最適化のために確率的勾配降下法を用いている．\n",
    "\n",
    "参考文献:[Bonanza#数式](https://ja.wikipedia.org/wiki/Bonanza#%E6%95%B0%E5%BC%8F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gdm6.png\" width=\"400\">\n",
    "<div style=\"text-align: center;\">\n",
    "図6.  最急降下法のイメージ\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 最急降下法を1変数関数に適用する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目的関数\n",
    "\n",
    "$$\n",
    "f(x) = x^4-8x^3+18x^2-3x+1\n",
    "$$\n",
    "\n",
    "この関数の最小値を求めたい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#\n",
    "# 目的関数を設定（この関数の最小値を求めたい．）\n",
    "#\n",
    "def mathfunc(x):\n",
    "    y = x**4 - 8*x**3 + 18*x**2 - 3*x  + 1\n",
    "    return y\n",
    "\n",
    "#\n",
    "# 作成した関数のグラフを表示する\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# グラフ描画のためのデータ作成\n",
    "xmin = -1.0\n",
    "xmax = 4.5\n",
    "num = 100\n",
    "x = np.linspace(xmin, xmax, num)\n",
    "y = mathfunc(x)\n",
    "\n",
    "# グラフ描画\n",
    "plt.plot(x, y)\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### この目的関数の最小値を最急降下法で求める\n",
    "目的関数を微分する．これは人間が解く必要がある．  \n",
    "\n",
    "ただし，近年は，有用なツールが多く存在する．  \n",
    "例えば，[Wolfram](https://www.wolframalpha.com/)を使えば，グラフの表示だけでなく，微分もしてくれる．\n",
    "\n",
    "\n",
    "$$\n",
    " f(x)=x^4 - 8x^3 + 18x^2 - 3x - 11\\\\\n",
    " f'(x) = 4x^3-24x^2+36x-3\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期値\n",
    "a = 2.0\n",
    "\n",
    "# 学習係数\n",
    "eta = 0.01\n",
    "\n",
    "# 目的関数の微分\n",
    "def ｄerivative(x):\n",
    "    y = 4*x**3 - 24*x**2 + 36*x-3\n",
    "    return y\n",
    "\n",
    "for i in range(10):\n",
    "    print(a)\n",
    "\n",
    "    # 更新式\n",
    "    a = a - eta*derivative(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初期値2．0で初めて，徐々に最小値をとるｘに近づいている．  \n",
    "これをグラフ表示する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#\n",
    "# 目的関数を設定（この関数の最小値を求めたい．）\n",
    "#\n",
    "def mathfunc(x):\n",
    "    y = x**4 - 8*x**3 + 18*x**2 - 3*x +1\n",
    "    return y\n",
    "\n",
    "# 最急降下法で目的関数の最小値をとるパラメータを求める．\n",
    "\n",
    "# 初期値\n",
    "a = 1.2\n",
    "\n",
    "# 学習係数\n",
    "eta = 0.01\n",
    "\n",
    "# 目的関数の微分\n",
    "def ｄerivative(x):\n",
    "    y = 4*x**3 - 24*x**2 + 36*x-3\n",
    "    return y\n",
    "\n",
    "para_x = []\n",
    "para_y =[]\n",
    "\n",
    "# 終了条件\n",
    "epsilon = 0.0001\n",
    "old_para = a\n",
    "\n",
    "ct = 0\n",
    "for i in range(1000):\n",
    "    para_x.append(a)\n",
    "    para_y.append(0)\n",
    "    # 更新式\n",
    "    a = a - eta*derivative(a)\n",
    "    if abs(old_para-a)<epsilon:\n",
    "        break\n",
    "    old_para = a\n",
    "    ct+=1\n",
    "\n",
    "# 更新回数\n",
    "print('更新回数=', ct)\n",
    "\n",
    "# 求めたパラメータ\n",
    "print('a=', a)\n",
    "\n",
    "#\n",
    "# 最急降下法の様子をグラフを表示する\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# グラフ描画のためのデータ作成\n",
    "xmin = -1.0\n",
    "xmax = 4.5\n",
    "num = 100\n",
    "x = np.linspace(xmin, xmax, num)\n",
    "y = mathfunc(x)\n",
    "\n",
    "# グラフ描画\n",
    "plt.plot(x, y)\n",
    "plt.scatter(para_x, para_y, color='red')\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初期値を変更すると，求まる解が異なることがわかる．  \n",
    "また，一般的に終了条件は，回数ではなく，前回と今回の差が一定よりも小さくなったら終了する．  \n",
    "つまり，更新しても，パラメータが変更されなくなったら終了する．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習\n",
    "\n",
    "- 初期値a = -0.5を変更してみる．\n",
    "- 初期値a = 2.5を変更してみる．\n",
    "- 初期値a = 2.4を変更してみる．\n",
    "- 学習係数eta = 0.0001を変更してみる．\n",
    "- 終了条件epsilon = 0.00001を変更してみる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最急降下法を多変数関数に適用する\n",
    "目的関数が微分できれば，多変数関数でも極小値を求めることができる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目的関数\n",
    "\n",
    "$$\n",
    " f\\left(x,y\\right)=\\sin{(x)}+2x\\cos{(y)}+0.2x^2+0.2y^2\n",
    "$$\n",
    "\n",
    "の最小値を求めたい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もし，お手持ちのPCにgnuplotがインストールされていれば，マウスで局面をグリグリ動かすことができます．\n",
    "\n",
    "gnuplotのコマンド\n",
    "\n",
    "```\n",
    "set isosamples 50\n",
    "set hidden3d\n",
    "splot sin(x)+2*x*cos(y)+0.2*x**2+0.2*y**2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表示すると，次のような複雑な局面を持つ2変数関数です．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./img/sincos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多変数関数でも，偏微分できれば最急降下法が使えます．\n",
    "\n",
    "\n",
    "目的関数\n",
    "\n",
    "$$\n",
    " f\\left(x,y\\right)=\\sin{(x)}+2x\\cos{(y)}+0.2x^2+0.2y^2\n",
    "$$\n",
    "\n",
    "偏微分\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial f(x,y)}{\\partial x} &=\\cos{(x)}+2\\cos{(y)}+0.4x\\\\\n",
    "\\frac{\\partial f(x,y)}{\\partial y} &=-2x\\sin{(y)}+0.4y\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複雑な3次元データの最小値を求める．\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#\n",
    "# 目的関数を設定（この関数の最小値を求めたい．）\n",
    "#\n",
    "def mathfunc(a, b):\n",
    "    y = np.sin(a)+2*a*np.cos(b)+0.2*a**2+0.2*b**2\n",
    "    return y\n",
    "\n",
    "# 最急降下法で目的関数の最小値をとるパラメータを求める．\n",
    "\n",
    "# 初期値\n",
    "a = 5.8\n",
    "b = 6.2\n",
    "c = mathfunc(a, b)\n",
    "\n",
    "# 学習係数\n",
    "eta = 0.01\n",
    "\n",
    "# 目的関数の偏微分:x\n",
    "def ｄerivative_x(a, b):\n",
    "    y = np.cos(a)+2.0*np.cos(b)+0.4*a\n",
    "    return y\n",
    "\n",
    "# 目的関数の偏微分:y\n",
    "def ｄerivative_y(a,b):\n",
    "    y = -2*np.sin(b)+0.4*b\n",
    "    return y\n",
    "\n",
    "para_a = []\n",
    "para_b =[]\n",
    "para_c =[]\n",
    "\n",
    "# 終了条件\n",
    "epsilon = 0.0001\n",
    "old_para_a = a\n",
    "old_para_b = b\n",
    "\n",
    "ct = 0\n",
    "for i in range(1000):\n",
    "    para_a.append(a)\n",
    "    para_b.append(b)\n",
    "    para_c.append(c)\n",
    "\n",
    "    # 更新式\n",
    "    a = a - eta*derivative_x(a, b)\n",
    "    b = b - eta*derivative_y(a, b)\n",
    "    c = mathfunc(a, b)\n",
    "    \n",
    "    if abs(old_para_a-a)<epsilon and abs(old_para_b-b)<epsilon:\n",
    "        break\n",
    "    old_para_a = a\n",
    "    old_para_b = b\n",
    "    \n",
    "    ct+=1\n",
    "\n",
    "# 更新回数\n",
    "print('更新回数=', ct)\n",
    "\n",
    "# 求めたパラメータ\n",
    "print('a=', a)\n",
    "print('b=', b)\n",
    "\n",
    "#\n",
    "# 最急降下法の様子をグラフを表示する\n",
    "# （ここでは3次元グラフ）\n",
    "#\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(-10.0, 10.0, 1)\n",
    "y = np.arange(-10.0, 10.0, 1)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = mathfunc(X, Y)\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.plot_wireframe(X, Y, Z)\n",
    "ax.plot(para_a, para_b, para_c,marker=\"o\",linestyle='None', color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このままでは本当に最小値を見つけたわかりにくいので，ファイル出力して，gnuplotでグラフを動かしてみる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# わかりにくいのでデータをファイル出力して，gnuplotで表示する\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "x = np.array(para_a).T\n",
    "df_x = pd.DataFrame(x)\n",
    "\n",
    "y = np.array(para_b).T\n",
    "df_y = pd.DataFrame(y)\n",
    "\n",
    "z = np.array(para_c).T\n",
    "df_z = pd.DataFrame(z)\n",
    "\n",
    "df = pd.concat([df_x, df_y], axis=1)\n",
    "df = pd.concat([df, df_z], axis=1)\n",
    "df.to_csv(\"sincos.csv\",header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gnuplotのコマンド\n",
    "\n",
    "set datafile separator \",\"  \n",
    "set isosamples 30  \n",
    "splot [0:10][0:10]sin(x)+2*x*cos(y)+0.2*x**2+0.2*y**2,\"sincos.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最急降下法の欠点\n",
    "- 最急降下法は最小値ではなく，極小値を求めることしかできない．  \n",
    "- 結果が初期値に依存する\n",
    "- 学習係数のパラメータの設定を行う必要がある．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
